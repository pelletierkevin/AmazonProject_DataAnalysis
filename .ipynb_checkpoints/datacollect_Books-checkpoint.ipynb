{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Collection</h1>\n",
    "\n",
    "Amazon Data : Collection/Analysis of data from Amazon (Reviews and Metadata of products) associated to the thema : ecology, bio, renewable etc... (see keywords vector)\n",
    "\n",
    "__Review data__ : \n",
    "Download http://jmcauley.ucsd.edu/data/amazon/links.html go to 'Per Category Files' section and DL 'reviews' file for a \n",
    "chosen category. (Or the Complete Review Data 18GB) It is better to not take the 5-core data as it contains only 5 reviews for each products (we're missing data)\n",
    "\n",
    "Features:\n",
    "\n",
    "- reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- reviewerName - name of the reviewer\n",
    "- helpful - helpfulness rating of the review, e.g. 2/3\n",
    "- reviewText - text of the review\n",
    "- overall - rating of the product\n",
    "- summary - summary of the review\n",
    "- unixReviewTime - time of the review (unix time)\n",
    "- reviewTime - time of the review (raw)\n",
    "\n",
    "__Metadata (Product)__ : \n",
    "Download http://jmcauley.ucsd.edu/data/amazon/links.html go to 'Per Category Files' section and DL 'metadata' file for a chosen category. (Or the Complete Review Data 18GB) It is better to not take the 5-core data as it contains only 5 reviews for each products (we're missing data)\n",
    "\n",
    "\n",
    "Features:\n",
    "\n",
    "- asin - ID of the product, e.g. 0000031852\n",
    "- title - name of the product\n",
    "- price - price in US dollars (at time of crawl)\n",
    "- imUrl - url of the product image\n",
    "- related - related products (also bought, also viewed, bought together, buy after viewing)\n",
    "- salesRank - sales rank information\n",
    "- brand - brand name\n",
    "- categories - list of categories the product belongs to\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TO DO LIST </h3>\n",
    "\n",
    "In the followings statements, 'extracted' means filtered with the thema : products/reviews associated to bio/ecology/renewable etc...\n",
    "\n",
    "- Choose category to focus on : Books ? / Health Care ....\n",
    "- Show Proportion of extracted data compared to the full data (of the category)\n",
    "- Comparison of extracted data between categories (which one contains the most related products/review interest)\n",
    "- Distribution Price of extracted data (per category) --> Compare between category and with not extracted data in the same category\n",
    "- Distribution salesRank of extracted data (best per category, mean, proportion of 10% first, 20%first etc...) --> Compare between category and with not extracted data in the same category\n",
    "\n",
    "- __Keywords__ : How to efficiently implement the selection of related data ? Currently with a list of key words, can use regexp, better writing of keywords etc.. ?\n",
    "\n",
    "- Associate Metadata with Reviews : __Join__\n",
    "> - Extract year of 1st review (which will give the publication year of the product -approximately-)\n",
    "> - Histograms number of extracted products per year \n",
    "> - Histograms number of extracted reviews per year\n",
    "> - Nb of reviews per product per category --> Distribution, compare between categories and with not extracted data to see if reviewers are more inerested/active with our thema products compared to others\n",
    "> - Helplful note : Compare between categories / Overall\n",
    "> - Mean rating product : Compare with other products\n",
    "\n",
    "\n",
    "- __Prediction__\n",
    "> - Nb of products for following years (Linear Regression)\n",
    "> - Sentiment Analysis on Reviews (Experiment ?)\n",
    "> - Prediction overall on price for following years ? (LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder named 'data' and store the downloaded json zip file. Then unzip in there to get the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\" global warming\", \" solar energy\", \" recycling \", \" pollution \", \"solar power\", \" endangered species\", \"air pollution\", \\\n",
    "\" water pollution\", \" wind energy\", \" climate change\", \" wind power\", \" recycle \", \" deforestation\", \" greenhouse effect\", \"environment\", \\\n",
    "\" sustainability \", \" natural resources\", \"alternative energy\", \" climate \", \"global warming\", \"renewable energy\", \" ecology\", \"composting\", \\\n",
    "\" carbon footprint\", \" bio \", \" biosphere \", \" renewable \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load once the global json. You probably won't be able to store it in parquet directly because some features ('related' or 'categories' e.g) can't be written in a file as it is a specific array with weird names. \n",
    "You then to clean first the data and store it to parquet. If you want next to retrieve some deleted features ('related' for example) you better do the filtering and extraction on the cleaned dataset and then merge or filter the global dataset using the new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __SAMPLE WITH THE CATEGORY HEALTH AND PERSONAL CARE METADATA__\n",
    "\n",
    "Download the meta_Health_and_Personal_Care.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_products = spark.read.json(DATA_DIR+\"meta_Books.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_corrupt_record=None, asin='0001048791', brand=None, categories=[['Books']], description=None, imUrl='http://ecx.images-amazon.com/images/I/51MKP0T4DBL.jpg', price=None, related=None, salesRank=Row(Arts, Crafts & Sewing=None, Books=6334800, Cell Phones & Accessories=None, Clothing=None, Electronics=None, Health & Personal Care=None, Home &amp; Kitchen=None, Industrial & Scientific=None, Jewelry=None, Kitchen & Dining=None, Movies & TV=None, Music=None, Musical Instruments=None, Office Products=None, Shoes=None, Sports &amp; Outdoors=None, Toys & Games=None), title='The Crucible: Performed by Stuart Pankin, Jerome Dempsey &amp; Cast')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see an example of the data\n",
    "meta_products.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the IDs for each feature in order to extract them in some filtering/flatmap functions with spark\n",
    "\n",
    "Example: lambda r: r[0], r[1] etc...\n",
    "\n",
    "- _corrupt_record 0\n",
    "- asin 1\n",
    "- brand 2\n",
    "- categories 3\n",
    "- description 4\n",
    "- imUrl 5\n",
    "- price 6 \n",
    "- related 7 \n",
    "- salesRank 8\n",
    "- title 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The next code will extract the relevant and writtable features. It will then store it into parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will extract only the features and turn them into more readable features.\n",
    "# Filter salesRank = None because this will lead to problems for the writing in parquet\n",
    "# Features removed : corruptRecord, imURL, related\n",
    "data_cleaned = meta_products.rdd.flatMap(lambda r: [(r[1], r[2], 'Books', r[4], r[6],  r[9] )]) \n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StructType to define the DataFrame that we'll create with the previously extracted rdd table\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    #StructField(\"salesRank\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform the RDD data into DataFrame (we'll then be able to store it in Parquet)\n",
    "datacleaned_DF = spark.createDataFrame(data_cleaned, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+--------------------+-----+--------------------+\n",
      "|      asin|brand|category|         description|price|               title|\n",
      "+----------+-----+--------+--------------------+-----+--------------------+\n",
      "|0001048791| null|   Books|                null| null|The Crucible: Per...|\n",
      "|0001048775| null|   Books|William Shakespea...| null|Measure for Measu...|\n",
      "|0001048236| null|   Books|&#34;One thing is...| 9.26|The Sherlock Holm...|\n",
      "+----------+-----+--------+--------------------+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of the cleaned data (and association with the created StrucType schema)\n",
    "datacleaned_DF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save into parquet to save time in the next times\n",
    "datacleaned_DF.write.mode('overwrite').parquet(\"meta_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the parquet data\n",
    "datacleaned_DF = spark.read.parquet(\"meta_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin='1453610596', brand=None, category='Books', description=None, price=1.9900000095367432, title='The Complete Works of Ralph Waldo Emerson &amp; Henry David Thoreau')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to see the structure of data (The StrucType schema is indeed there)\n",
    "datacleaned_DF.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created the new dataset here are the features ID of this new one.\n",
    "\n",
    "These are the IDs for each feature in order to extract them in some filtering/flatmap functions with spark\n",
    "Example: lambda r: r[0], r[1] etc...\n",
    "\n",
    "- asin 0\n",
    "- brand 1\n",
    "- category 2\n",
    "- description 3\n",
    "- price 4 \n",
    "- title 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples : 2370585\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data samples : \" + str(datacleaned_DF.rdd.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will now extract the data related to our thema : Ecology, Bio, Renewable etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with title and description not equal to None\n",
    "# We will then be able to test if those features contains words defined in the keyword vector \n",
    "# The keyword vector represents the thema that we want : ecology, bio etc...\n",
    "filter_products_bio = datacleaned_DF.rdd.filter(lambda r: (r[5] != None) &  (r[3] != None)) \\\n",
    "                    .filter(lambda r: (any(word in r[5].lower() for word in keywords)) | (any(word in r[3].lower() for word in keywords)) ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples related to ecology/bio/renewable etc... : 20723\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data samples related to ecology/bio/renewable etc... : \" + str(filter_products_bio.count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin='1453618805', brand=None, category='Books', description=\"Edmund Kealoha Parker Sr. was born on March 19, 1931, in Honolulu, Hawaii.  Growing up in the harsh Honolulu environment, young Ed had many opportunities to defend himself, his principles, and his honor. Ed attended Kamehameha High School, he excelled in sports, becoming an accomplished athlete. He became a judo black belt by the age of fifteen, as well as an amateur boxing champion. He competed in track and field, and became a member of the Junior Varsity Football Team.  Ed was born and raised in a devout Mormon household. He graduated from Kamehameha High School served his country in the Coast Guard for three years. He worked hard to achieve his Black Belt Certificate from Professor William K. S. Chow June 5, 1953. During his high school and young adult years he dated Marguerite Leilani Bertelmann Yap. He graduated from Brigham Young University and married Marguerite Leilani Bertelmann Yap in December 28, 1954 in the Church of Jesus Christ of  Latter-day Saints, Salt Lake, Utah Temple. Edmund opened the first Karate Studio in Pasadena, California in 1956. Karate was unknown in California. Terry Robinson a close friend who owned a private health club in Beverly Hills, the Beverly Wilshire Health Club. Introduce Ed to many celebrities whom Ed taught, Bob Wagner, Natalie Word, Mac Donald Carey, Elkie Sommers, Blake Edwards, Frank Lovejoy, Audy Murphy, Nick Adams, Gary Cooper and Elvis Presley.  Ed when meeting and becoming close friends with Bruce Lee introduced Bruce to the directors who eventually cast Bruce Lee in the TV show the Green Hornet. Ed appeared in many movies and television shows, including Revenge of the Pink Panther and The Curse of the Pink Panther. He also was on The Lucy Show where he taught Lucille Ball and Ethel Mertz. He was in The Courtship of Eddie's Father and I Spy.  Ed taught many thousands all over the World. He frequently put on Seminars and Demonstrations in Australia, England, Ireland, Spain, Chile, Venezuela, Sweden, Holland and Greece. For many years Mr. Parker authored many books which still sell very well all over the World. Basic Booklet on Karate, Kenpo Karate Law of the Fist, The Women's Guide to Self Defense, Secrets of Chinese Karate, Infinite Insights into Kenpo 1-5 volumes, The Zen of Kenpo, The Encyclopedia of Kenpo, & Guide to the Nunchaku. Ed Parker's true Aloha spirit is truly missed all over the world. Please check out his website at: www.edparkersr.com\", price=13.5, title='Kenpo Karate: Law of the Fist and the Empty Hand'),\n",
       " Row(asin='1453626751', brand=None, category='Books', description=\"What I tried to accomplish in this book is to show that one's life is affected as much by circumstances as by emotions. My professional books dealt with the ways the social environment affected behavior.&#xA0; By this, I mean if you pay someone to look the other way, there is a good chance they will look the other way. But in my own life I found that I was affected by many more factors: chance encounters, whims, love, uncertain feelings and convictions. Rewards, pay, gain yes, I paid some attention to them but love and sex was as crucial. Sometimes I did not understand why I was doing what I was doing.From Paris to Berkeleyattempts to tell you that slowly, amusingly and persuasively.\", price=9.989999771118164, title='From Paris to Berkeley: Memoir'),\n",
       " Row(asin='1453636196', brand=None, category='Books', description='West Australian born Judith Evered received a BA with honors from the University of West Australia and did research on accident proneness in children at Perth Children&#x2019;s Hospital. As a student, she played competitive tennis and crossed the Australian desert, Nullarbor, to play intervarsity and interstate hockey.  In 1951, she moved to London and continued to live communally while at the Women&#x2019;s International Crosby Hall. She met American, German, Finish and other folk as she explored Europe and interviewed for public opinion around the English countryside. After enrolling in the University of London&#x2019;s School of Economics, she met her future husband Roger Evered. In 1961, Roger accepted a research teaching position at the University of Michigan Ann Arbor and the family moved to the U.S. In 1963, they relocated to Newport Beach, California when Roger took a job in Long Beach. At this time, Judith co-led a course entitled developing personal potential through the UCLA extension and studied practical media production at Long Beach Community College as well as early childhood development at Orange Coast College. After moving to Santa Barabra, Judith completed an MA in confluent education at UCSB. She has been involved in community organizing for peace and freedom since the early 1970&#x2019;s and participated in marches for peace and justice in several U.S. cities. As a member of the Isla Vista Recreation and Park Board for three terms, she helped procure $1 million for new parks in Isla Vista. She also initiated the non-profit organization POISE (People for Open, Informal Self -directed Education) with a group of parents who petitioned the Goleta School Board to begin an open classroom. Judith has performed in street theater as Judge Judy in a trial of George Bush for murder and other related crimes. The 80 jurists formed by the audience unanimously voted him guilty.  Judith, a psychologist and educator by nature has a strong sense of solidarity with the Women&#x2019;s International League for Peace and Freedom, the Mothers for Peace and the Vandenberg Action Coalition. Sustainability is close to Judith&#x2019;s heart. She is qualified to teach Permaculture and enjoys sharing information for healthy habits, exercise and eating locally grown, organic foods. Her strength and passion will be forever in the memories of those fortunate enough to have made her acquaintance. She is blessed with three sons, their wives, five granddaughters and one grandson.', price=13.5, title='Protest Diablo: Living and Dying Under the Shadow of a Nuclear Power Plant')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_products_bio.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    #StructField(\"salesRank\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the RDD data into DataFrame (we'll then be able work and join with review data)\n",
    "DF_filter_products_bio = spark.createDataFrame(filter_products_bio, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __SAMPLE WITH THE CATEGORY HEALTH AND PERSONAL CARE REVIEWS__\n",
    "\n",
    "Download the reviews_Health_and_Personal_Care.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = spark.read.json(DATA_DIR+\"reviews_Books.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin='0000000116', helpful=[5, 5], overall=4.0, reviewText=\"Interesting Grisham tale of a lawyer that takes millions of dollars from his firm after faking his own death. Grisham usually is able to hook his readers early and ,in this case, doesn't play his hand to soon. The usually reliable Frank Mueller makes this story even an even better bet on Audiobook.\", reviewTime='04 27, 2002', reviewerID='AH2L9G3DQHHAJ', reviewerName='chris', summary='Show me the money!', unixReviewTime=1019865600)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see an example of the data\n",
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the IDs for each feature in order to extract them in some filtering/flatmap functions with spark\n",
    "\n",
    "Example: lambda r: r[0], r[1] etc...\n",
    "\n",
    "- asin 0\n",
    "- helpful 1\n",
    "- overall 2\n",
    "- reviewText 3\n",
    "- reviewTime 4\n",
    "- reviewerID 5\n",
    "- reviewerName 6 \n",
    "- summary 7 \n",
    "- unixReviewTime 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save into parquet to save time in the next times\n",
    "reviews.write.mode('overwrite').parquet(\"reviews_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the parquet data\n",
    "reviews = spark.read.parquet(\"reviews_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 16, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Programmes\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 238, in main\n  File \"D:\\Programmes\\Spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 690, in read_int\n    length = stream.read(4)\n  File \"D:\\Anaconda3\\envs\\ada\\lib\\socket.py\", line 576, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Programmes\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 238, in main\n  File \"D:\\Programmes\\Spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 690, in read_int\n    length = stream.read(4)\n  File \"D:\\Anaconda3\\envs\\ada\\lib\\socket.py\", line 576, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f1da1d3ea01f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of reviews : \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programmes\\Spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \"\"\"\n\u001b[1;32m-> 1053\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \"\"\"\n\u001b[1;32m-> 1044\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \"\"\"\n\u001b[0;32m    813\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\Spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 16, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Programmes\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 238, in main\n  File \"D:\\Programmes\\Spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 690, in read_int\n    length = stream.read(4)\n  File \"D:\\Anaconda3\\envs\\ada\\lib\\socket.py\", line 576, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Programmes\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 238, in main\n  File \"D:\\Programmes\\Spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 690, in read_int\n    length = stream.read(4)\n  File \"D:\\Anaconda3\\envs\\ada\\lib\\socket.py\", line 576, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reviews : \" + str(reviews.rdd.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Join Reviews and Metadata__ \n",
    "\n",
    "We will now join the metadata and review dataset using the product ID. Then we'll have the reviews for all products concerned. Here we will join with the filtered data containing the products related to the thema 'bio,recycle,ecology' etc... \n",
    "\n",
    "Then in each row we will have the product description and the reviews associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This way allows no duplicate\n",
    "review_product_join = DF_filter_products_bio.join(reviews, ['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of reviews related to bio/renewable/ecology etc... : \" + str(review_product_join.rdd.count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of joined data\n",
    "review_product_join.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_product_join.write.mode('overwrite').parquet(\"joined_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sqlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstreview = review_product_join.groupBy(\"asin\").agg(sqlf.min(\"unixReviewTime\"))\n",
    "#.min('unixReviewTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstreview.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_reviews_Pandas = firstreview.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_reviews_Pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_reviews_Pandas['Year'] = pd.to_datetime(year_reviews_Pandas['min(unixReviewTime)'],unit='s').map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year_reviews_Pandas.hist(\"Year\", color=\"lime\", log=True,bins=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of nb of reviews /year linked to products bio/renwable etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalfirstreview = reviews.groupBy(\"asin\").agg(sqlf.min(\"unixReviewTime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalfirstreview.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalyear_reviews_Pandas = globalfirstreview.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalyear_reviews_Pandas['Year'] = pd.to_datetime(globalyear_reviews_Pandas['min(unixReviewTime)'],unit='s').map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalyear_reviews_Pandas.hist(\"Year\", log=True, color=\"sienna\", bins = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the merged dataframe that there is the features from metadata about the products, and the features about the reviews. We will then describe the features ID : \n",
    "\n",
    "Example: lambda r: r[0], r[1] etc...\n",
    "\n",
    "- asin 0\n",
    "- brand 1\n",
    "- category 2\n",
    "- description 3\n",
    "- price 4\n",
    "- salesRank 5\n",
    "- title 6\n",
    "- helpful 7 \n",
    "- overall 8\n",
    "- reviewText 9\n",
    "- reviewTime 10\n",
    "- reviewerID 11\n",
    "- reviewerName 12\n",
    "- summary 13\n",
    "- unixReviewTime 14\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbreviews_perproduct=  review_product_join.rdd.map(lambda r: [r[0],1]) \\\n",
    "            .reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbreviews_DF = spark.createDataFrame(nbreviews_perproduct, ['productID','freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbreviews_Pandas = nbreviews_DF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Nb of reviews/ Product Distribution Associated to Ecology/Bio etc..')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True)\n",
    "fig.set_size_inches(16,4)\n",
    "\n",
    "nbreviews_Pandas.hist(log=True, bins = 100, color=\"coral\", cumulative=-1,figsize =(16,4), ax=axes)\n",
    "\n",
    "axes.set_xlabel(\"Nb of reviews/ Product\")\n",
    "axes.set_ylabel(\"Frequency\")\n",
    "axes.set_title(\"Nb of reviews/ Product Distribution Associated to Ecology/Bio etc..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
