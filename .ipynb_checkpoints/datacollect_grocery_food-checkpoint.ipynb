{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Collection</h1>\n",
    "\n",
    "Amazon Data : Collection/Analysis of data from Amazon (Reviews and Metadata of products) associated to the thema : ecology, bio, renewable etc... (see keywords vector)\n",
    "\n",
    "__Review data__ : \n",
    "Download http://jmcauley.ucsd.edu/data/amazon/links.html go to 'Per Category Files' section and DL 'reviews' file for a \n",
    "chosen category. (Or the Complete Review Data 18GB) It is better to not take the 5-core data as it contains only 5 reviews for each products (we're missing data)\n",
    "\n",
    "Features:\n",
    "\n",
    "- reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- reviewerName - name of the reviewer\n",
    "- helpful - helpfulness rating of the review, e.g. 2/3\n",
    "- reviewText - text of the review\n",
    "- overall - rating of the product\n",
    "- summary - summary of the review\n",
    "- unixReviewTime - time of the review (unix time)\n",
    "- reviewTime - time of the review (raw)\n",
    "\n",
    "__Metadata (Product)__ : \n",
    "Download http://jmcauley.ucsd.edu/data/amazon/links.html go to 'Per Category Files' section and DL 'metadata' file for a chosen category. (Or the Complete Review Data 18GB) It is better to not take the 5-core data as it contains only 5 reviews for each products (we're missing data)\n",
    "\n",
    "\n",
    "Features:\n",
    "\n",
    "- asin - ID of the product, e.g. 0000031852\n",
    "- title - name of the product\n",
    "- price - price in US dollars (at time of crawl)\n",
    "- imUrl - url of the product image\n",
    "- related - related products (also bought, also viewed, bought together, buy after viewing)\n",
    "- salesRank - sales rank information\n",
    "- brand - brand name\n",
    "- categories - list of categories the product belongs to\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TO DO LIST </h3>\n",
    "\n",
    "In the followings statements, 'extracted' means filtered with the thema : products/reviews associated to bio/ecology/renewable etc...\n",
    "\n",
    "- Choose category to focus on : Books ? / Health Care ....\n",
    "- Show Proportion of extracted data compared to the full data (of the category)\n",
    "- Comparison of extracted data between categories (which one contains the most related products/review interest)\n",
    "- Distribution Price of extracted data (per category) --> Compare between category and with not extracted data in the same category\n",
    "- Distribution salesRank of extracted data (best per category, mean, proportion of 10% first, 20%first etc...) --> Compare between category and with not extracted data in the same category\n",
    "\n",
    "- __Keywords__ : How to efficiently implement the selection of related data ? Currently with a list of key words, can use regexp, better writing of keywords etc.. ?\n",
    "\n",
    "- Associate Metadata with Reviews : __Join__\n",
    "> - Extract year of 1st review (which will give the publication year of the product -approximately-)\n",
    "> - Histograms number of extracted products per year \n",
    "> - Histograms number of extracted reviews per year\n",
    "> - Nb of reviews per product per category --> Distribution, compare between categories and with not extracted data to see if reviewers are more inerested/active with our thema products compared to others\n",
    "> - Helplful note : Compare between categories / Overall\n",
    "> - Mean rating product : Compare with other products\n",
    "\n",
    "\n",
    "- __Prediction__\n",
    "> - Nb of products for following years (Linear Regression)\n",
    "> - Sentiment Analysis on Reviews (Experiment ?)\n",
    "> - Prediction overall on price for following years ? (LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder named 'data' and store the downloaded json zip file. Then unzip in there to get the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\" global warming\", \" solar energy\", \" recycling \", \" pollution \", \"solar power\", \" endangered species\", \"air pollution\", \\\n",
    "\" water pollution\", \" wind energy\", \" climate change\", \" wind power\", \" recycle \", \" deforestation\", \" greenhouse effect\", \"environment\", \\\n",
    "\" sustainability \", \" natural resources\", \"alternative energy\", \" climate \", \"global warming\", \"renewable energy\", \" ecology\", \"composting\", \\\n",
    "\" carbon footprint\", \" bio \", \" biosphere \", \" renewable \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load once the global json. You probably won't be able to store it in parquet directly because some features ('related' or 'categories' e.g) can't be written in a file as it is a specific array with weird names. \n",
    "You then to clean first the data and store it to parquet. If you want next to retrieve some deleted features ('related' for example) you better do the filtering and extraction on the cleaned dataset and then merge or filter the global dataset using the new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __SAMPLE WITH THE CATEGORY HEALTH AND PERSONAL CARE METADATA__\n",
    "\n",
    "Download the meta_Health_and_Personal_Care.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_products = spark.read.json(DATA_DIR+\"meta_Grocery_and_Gourmet_Food.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin=None, brand=None, categories=None, description=None, imUrl=None, price=None, related=None, salesRank=None, title=None),\n",
       " Row(asin='0700026444', brand=None, categories=[['Grocery & Gourmet Food']], description='Silverpot Tea, Pure Darjeeling, is an exquisite tea enjoyed at leisure, when you want to relax or celebrate.   Its mellow yet layered taste will constantly surprise and delight.  This subtle and refined tea is of incomparable taste and flavor.  \\n\\nPackaged in an exotic handmade pinewood chestlet, this magnificent tea was rolled from tender leaves grown and hand plucked in the legendary mist covered fields of Darjeeling.\\n\\nFor authentic experience of this luxury tea, it is presented as 100 gms  loose leaf, and is a great way to show your appreciation, celebrate an occasion or send your best wishes.\\n\\nSilverpot is different - it embraces the pioneering spirit of the Ghose family, personifies the highest standards of excellence, and is proof that this is only possible from the love and care of a family tea company located at the country of origin, with a tea tradition of five generations.The Silverpot philosophy is, first and foremost, about a passionatecommitment to quality and a series of uncompromising choices. These, taken together, create a taste anda style that is as legendary as it is unique. Silverpot Tea balances richness, freshness andfinesse, and is nurtured from the leaf to the cup with painstaking care and attention to detail.HERITAGE.Silverpot is born of a cherished heritage dating back to 1879, when tea was still a monopoly of the colonial rulers, and the Ghose family established Indias first native owned tea plantation. The legacy of Debes Chandra Ghose and his family continues till this day. Each expression of Silverpot draws on the companys long history, rich traditions and legendary dedication to quality teas.', imUrl='http://ecx.images-amazon.com/images/I/51hs8sox%2BJL._SY300_.jpg', price=None, related=None, salesRank=Row(Arts, Crafts & Sewing=None, Automotive=None, Beauty=None, Camera &amp; Photo=None, Cell Phones & Accessories=None, Clothing=None, Computers & Accessories=None, Electronics=None, Grocery & Gourmet Food=620307, Health & Personal Care=None, Home &amp; Kitchen=None, Home Improvement=None, Industrial & Scientific=None, Jewelry=None, Kitchen & Dining=None, Movies & TV=None, Music=None, Office Products=None, Patio, Lawn & Garden=None, Pet Supplies=None, Shoes=None, Software=None, Sports &amp; Outdoors=None, Toys & Games=None, Video Games=None), title='Pure Darjeeling Tea: Loose Leaf')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see an example of the data\n",
    "meta_products.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, asin: string, brand: string, description: string, imUrl: string, price: string, title: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_products.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the IDs for each feature in order to extract them in some filtering/flatmap functions with spark\n",
    "\n",
    "Example: lambda r: r[0], r[1] etc...\n",
    "\n",
    "- asin 0\n",
    "- brand 1\n",
    "- categories 2\n",
    "- description 3\n",
    "- imUrl 4\n",
    "- price 5\n",
    "- related 6\n",
    "- salesRank 7\n",
    "- title 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The next code will extract the relevant and writtable features. It will then store it into parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will extract only the features and turn them into more readable features.\n",
    "# Filter salesRank = None because this will lead to problems for the writing in parquet\n",
    "# Features removed : corruptRecord, imURL, related\n",
    "data_cleaned = meta_products.rdd.filter(lambda r: (r[7] != None ) )  \\\n",
    "                    .flatMap(lambda r: [(r[0], r[1], r[2][0][0], r[3], r[5],r[7]['Grocery & Gourmet Food'],  r[8] )]) \\\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StructType to define the DataFrame that we'll create with the previously extracted rdd table\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"salesRank\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform the RDD data into DataFrame (we'll then be able to store it in Parquet)\n",
    "datacleaned_DF = spark.createDataFrame(data_cleaned, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+--------------------+-----+---------+--------------------+\n",
      "|      asin|brand|            category|         description|price|salesRank|               title|\n",
      "+----------+-----+--------------------+--------------------+-----+---------+--------------------+\n",
      "|0700026444| null|Grocery & Gourmet...|Silverpot Tea, Pu...| null|   620307|Pure Darjeeling T...|\n",
      "|141278509X| null|Grocery & Gourmet...|Infused with Vita...| null|   620322|Archer Farms Stra...|\n",
      "|1453060375|  Mio|Grocery & Gourmet...|MiO Energy is you...|11.99|   268754|Mio Energy Liquid...|\n",
      "+----------+-----+--------------------+--------------------+-----+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of the cleaned data (and association with the created StrucType schema)\n",
    "datacleaned_DF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save into parquet to save time in the next times\n",
    "datacleaned_DF.write.mode('overwrite').parquet(\"meta_Grocery_and_Gourmet_Food.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the parquet data\n",
    "datacleaned_DF = spark.read.parquet(\"meta_Grocery_and_Gourmet_Food.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin='B001EO5U92', brand=None, category='Grocery & Gourmet Food', description=\"Naturally nutritious. Made with organic rice flour. Wheat free. Gluten free, a special blend. For those of us on gluten free diets, this mix is ideal. Enjoy pancakes hot off the griddle or flavorful muffins from your oven whenever you choose. We're sure you'll agree that we've made it easy and satisfying! From America's heartland to your heart. Arrowhead Mills has been the pioneer and leader in organic baking mixes, grains, cereals and nut butters sine 1960. We believe in nature's abundance and treat food with respect - not chemicals! Capturing the essence if the earth with organically grown ingredients, Arrowhead Mills takes you back to the basics with the best-tasting, most diverse selection of products for home-baked goodness. Made with organic rice flour. Good source of calcium. No preservatives. Made with no genetically engineered ingredients. Certified organic by the Texas Department of Agriculture (TDA).\", price=None, salesRank=166472, title='Arrowhead Mills Gluten-Free Pancake &amp; Baking Mix,  28-Ounce Packages (Pack of 3)')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to see the structure of data (The StrucType schema is indeed there)\n",
    "datacleaned_DF.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created the new dataset here are the features ID of this new one.\n",
    "\n",
    "These are the IDs for each feature in order to extract them in some filtering/flatmap functions with spark\n",
    "Example: lambda r: r[0], r[1] etc...\n",
    "\n",
    "- asin 0\n",
    "- brand 1\n",
    "- category 2\n",
    "- description 3\n",
    "- price 4 \n",
    "- salesRank 5\n",
    "- title 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples : 145854\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data samples : \" + str(datacleaned_DF.rdd.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will now extract the data related to our thema : Ecology, Bio, Renewable etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with title and description not equal to None\n",
    "# We will then be able to test if those features contains words defined in the keyword vector \n",
    "# The keyword vector represents the thema that we want : ecology, bio etc...\n",
    "filter_products_bio = datacleaned_DF.rdd.filter(lambda r: (r[6] != None) &  (r[3] != None)) \\\n",
    "                    .filter(lambda r: (any(word in r[6].lower() for word in keywords)) | (any(word in r[3].lower() for word in keywords)) ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of data samples related to ecology/bio/renewable etc... : \" + str(filter_products_bio.count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_products_bio.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the RDD data into DataFrame (we'll then be able work and join with review data)\n",
    "DF_filter_products_bio = spark.createDataFrame(filter_products_bio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __SAMPLE WITH THE CATEGORY HEALTH AND PERSONAL CARE REVIEWS__\n",
    "\n",
    "Download the reviews_Health_and_Personal_Care.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = spark.read.json(DATA_DIR+\"reviews_Grocery_and_Gourmet_Food.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see an example of the data\n",
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the IDs for each feature in order to extract them in some filtering/flatmap functions with spark\n",
    "\n",
    "Example: lambda r: r[0], r[1] etc...\n",
    "\n",
    "- asin 0\n",
    "- helpful 1\n",
    "- overall 2\n",
    "- reviewText 3\n",
    "- reviewTime 4\n",
    "- reviewerID 5\n",
    "- reviewerName 6 \n",
    "- summary 7 \n",
    "- unixReviewTime 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save into parquet to save time in the next times\n",
    "reviews.write.mode('overwrite').parquet(\"reviews_Grocery_and_Gourmet_Food.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the parquet data\n",
    "reviews = spark.read.parquet(\"reviews_Grocery_and_Gourmet_Food.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of reviews : \" + str(reviews.rdd.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Join Reviews and Metadata__ \n",
    "\n",
    "We will now join the metadata and review dataset using the product ID. Then we'll have the reviews for all products concerned. Here we will join with the filtered data containing the products related to the thema 'bio,recycle,ecology' etc... \n",
    "\n",
    "Then in each row we will have the product description and the reviews associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This way allows no duplicate\n",
    "review_product_join = DF_filter_products_bio.join(reviews, ['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of reviews related to bio/renewable/ecology etc... : \" + str(review_product_join.rdd.count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of joined data\n",
    "review_product_join.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minUnixTime(accum, n):\n",
    "    if(accum < n):\n",
    "        return accum\n",
    "    else:\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sqlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstreview = review_product_join.groupBy(\"asin\").agg(sqlf.min(\"unixReviewTime\"))\n",
    "#.min('unixReviewTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstreview.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_reviews_Pandas = firstreview.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_reviews_Pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_reviews_Pandas['Year'] = pd.to_datetime(year_reviews_Pandas['min(unixReviewTime)'],unit='s').map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year_reviews_Pandas.hist(\"Year\", color=\"lime\", log=True,bins=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of nb of reviews /year linked to products bio/renwable etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalfirstreview = reviews.groupBy(\"asin\").agg(sqlf.min(\"unixReviewTime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalfirstreview.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalyear_reviews_Pandas = globalfirstreview.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalyear_reviews_Pandas['Year'] = pd.to_datetime(globalyear_reviews_Pandas['min(unixReviewTime)'],unit='s').map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalyear_reviews_Pandas.hist(\"Year\", log=True, color=\"sienna\", bins = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the merged dataframe that there is the features from metadata about the products, and the features about the reviews. We will then describe the features ID : \n",
    "\n",
    "Example: lambda r: r[0], r[1] etc...\n",
    "\n",
    "- asin 0\n",
    "- brand 1\n",
    "- category 2\n",
    "- description 3\n",
    "- price 4\n",
    "- salesRank 5\n",
    "- title 6\n",
    "- helpful 7 \n",
    "- overall 8\n",
    "- reviewText 9\n",
    "- reviewTime 10\n",
    "- reviewerID 11\n",
    "- reviewerName 12\n",
    "- summary 13\n",
    "- unixReviewTime 14\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbreviews_perproduct=  review_product_join.rdd.map(lambda r: [r[0],1]) \\\n",
    "            .reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbreviews_DF = spark.createDataFrame(nbreviews_perproduct, ['productID','freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbreviews_Pandas = nbreviews_DF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True)\n",
    "fig.set_size_inches(16,4)\n",
    "\n",
    "nbreviews_Pandas.hist(log=True, bins = 100, color=\"coral\", cumulative=-1,figsize =(16,4), ax=axes)\n",
    "\n",
    "axes.set_xlabel(\"Nb of reviews/ Product\")\n",
    "axes.set_ylabel(\"Frequency\")\n",
    "axes.set_title(\"Nb of reviews/ Product Distribution Associated to Ecology/Bio etc..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
